{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNU6Mm96mXp6F1csJgUXeV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTBfPfUNl6Wf","executionInfo":{"status":"ok","timestamp":1707041431882,"user_tz":-330,"elapsed":697,"user":{"displayName":"Blah blah -","userId":"03567317729707022968"}},"outputId":"53d88dec-7337-41ff-df88-85b04c22e26d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Convergence reached after 24 iterations.\n","Final cost function value: 0.49359533746071566\n","Final learning parameter values (theta0, theta1): -4.961028781944015e-16 -0.10448338724926137\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","independent_data = pd.read_csv(\"linearX.csv\")\n","dependent_data = pd.read_csv(\"linearY.csv\")\n","X = independent_data.values.ravel()\n","y = dependent_data.values.ravel()\n","\n","\n","np.random.seed(0)\n","y_noisy = y + np.random.normal(0, 0.2, size=y.shape)\n","\n","X_mean = np.mean(X)\n","X_std = np.std(X)\n","X_normalized = (X - X_mean) / X_std\n","\n","y_mean = np.mean(y_noisy)\n","y_std = np.std(y_noisy)\n","y_normalized = (y_noisy - y_mean) / y_std\n","\n","\n","theta0 = 0\n","theta1 = 0\n","learning_rate = 0.1\n","iterations = 10000\n","convergence_threshold = 1e-5\n","\n","def hypothesis(X, theta0, theta1):\n","    return theta0 + theta1 * X\n","\n","def cost_function(X, y, theta0, theta1):\n","    m = len(X)\n","    predictions = hypothesis(X, theta0, theta1)\n","    squared_errors = (predictions - y) ** 2\n","    J = (1/(2*m)) * np.sum(squared_errors)\n","    return J\n","\n","m = len(X_normalized)\n","previous_cost = cost_function(X_normalized, y_normalized, theta0, theta1)\n","for i in range(iterations):\n","    predictions = hypothesis(X_normalized, theta0, theta1)\n","    error = predictions - y_normalized\n","\n","    dtheta0 = (1/m) * np.sum(error)\n","    dtheta1 = (1/m) * np.sum(error * X_normalized)\n","\n","    theta0 -= learning_rate * dtheta0\n","    theta1 -= learning_rate * dtheta1\n","    current_cost = cost_function(X_normalized, y_normalized, theta0, theta1)\n","\n","    if abs(current_cost - previous_cost) < convergence_threshold:\n","        print(\"Convergence reached after\", i+1, \"iterations.\")\n","        break\n","\n","\n","    previous_cost = current_cost\n","\n","print(\"Final cost function value:\", current_cost)\n","print(\"Final learning parameter values (theta0, theta1):\", theta0, theta1)\n","\n","theta0_denormalized = (y_std * theta0) + y_mean - (theta1 * y_std * (X_mean / X_std))\n","theta1_denormalized = (theta1 * y_std) / X_std\n","\n"]}]}